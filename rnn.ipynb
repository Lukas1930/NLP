{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pos-data/en_ewt-train.conll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide train data path as an argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_conll_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos-data/en_ewt-train.conll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m##dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m##sys.argv[1])\u001b[39;00m\n\u001b[0;32m     75\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_data ])\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mread_conll_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     22\u001b[0m current_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m current_tags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     26\u001b[0m     line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pos-data/en_ewt-train.conll'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "PAD = \"PAD\"\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "torch.manual_seed(8446)\n",
    "\n",
    "def read_conll_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word(idx)\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print('Please provide train data path as an argument')\n",
    "    exit(1)\n",
    "\n",
    "train_data = read_conll_file('pos-data/en_ewt-train.conll')\n",
    "##dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "##sys.argv[1])\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = token_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_feats, train_labels = data2feats(train_data, token_vocab, label_vocab)\n",
    "\n",
    "\n",
    "# convert to batches\n",
    "num_batches = int(len(train_feats)/BATCH_SIZE)\n",
    "train_feats_batches = train_feats[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)\n",
    "train_labels_batches = train_labels[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)\n",
    "\n",
    "class TaggerModel(nn.Module):\n",
    "    def __init__(self, nwords, ntags):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create word embeddings\n",
    "        self.word_embedding = nn.Embedding(nwords, DIM_EMBEDDING)\n",
    "        # Create input dropout parameter\n",
    "        self.word_dropout = nn.Dropout(.2)\n",
    "        # Create LSTM parameters\n",
    "        self.rnn = nn.RNN(DIM_EMBEDDING, LSTM_HIDDEN, num_layers=1,\n",
    "                batch_first=True, bidirectional=False)\n",
    "        # Create output dropout parameter\n",
    "        self.rnn_output_dropout = nn.Dropout(.3)\n",
    "        # Create final matrix multiply parameters\n",
    "        self.hidden_to_tag = nn.Linear(LSTM_HIDDEN, ntags)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # Look up word vectors\n",
    "        word_vectors = self.word_embedding(sentences)\n",
    "        # Apply dropout\n",
    "        dropped_word_vectors = self.word_dropout(word_vectors)\n",
    "        rnn_out, _ = self.rnn(dropped_word_vectors, None)\n",
    "        # Apply dropout\n",
    "        rnn_out_dropped = self.rnn_output_dropout(rnn_out)\n",
    "        # Matrix multiply to get scores for each tag\n",
    "        output_scores = self.hidden_to_tag(rnn_out_dropped)\n",
    "\n",
    "        # Calculate loss and predictions\n",
    "        return output_scores\n",
    "\n",
    "# define the model\n",
    "model = TaggerModel(NWORDS, NTAGS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "print('model overview: ')\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "print('epoch   loss      Train acc.')\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() \n",
    "    model.zero_grad()\n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for batchIdx in range(0, num_batches):\n",
    "        output_scores = model.forward(train_feats_batches[batchIdx])\n",
    "        output_scores = output_scores.view(BATCH_SIZE * max_len, -1)\n",
    "        flat_labels = train_labels_batches[batchIdx].view(BATCH_SIZE * max_len)\n",
    "        batch_loss = loss_function(output_scores, flat_labels)\n",
    "\n",
    "        predicted_labels = torch.argmax(output_scores, 1)\n",
    "        predicted_labels = predicted_labels.view(BATCH_SIZE, max_len)\n",
    "\n",
    "        # Run backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        loss += batch_loss.item()\n",
    "        # Update the number of correct tags and total tags\n",
    "        for gold_sent, pred_sent in zip(train_labels_batches[batchIdx], predicted_labels):\n",
    "            for gold_label, pred_label in zip(gold_sent, pred_sent):\n",
    "                if gold_label != 0:\n",
    "                    total += 1\n",
    "                    if gold_label == pred_label:\n",
    "                        match+= 1\n",
    "    print('{0: <8}{1: <10}{2}'.format(epoch, '{:.2f}'.format(loss/num_batches), '{:.4f}'.format(match / total)))\n",
    "\n",
    "\n",
    "def run_eval(feats_batches, labels_batches):\n",
    "    model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        output_scores = model.forward(sents)\n",
    "        predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate_on_test(model, test_data, token_vocab, label_vocab):\n",
    "    model.eval()\n",
    "    test_feats, test_labels = data2feats(test_data, token_vocab, label_vocab)\n",
    "    num_batches_test = len(test_feats) // BATCH_SIZE\n",
    "    test_feats_batches = test_feats[:BATCH_SIZE * num_batches_test].view(num_batches_test, BATCH_SIZE, max_len)\n",
    "    test_labels_batches = test_labels[:BATCH_SIZE * num_batches_test].view(num_batches_test, BATCH_SIZE, max_len)\n",
    "\n",
    "    # Run evaluation\n",
    "    total = 0\n",
    "    match = 0\n",
    "    for sents, labels in zip(test_feats_batches, test_labels_batches):\n",
    "        output_scores = model.forward(sents)\n",
    "        predicted_tags = torch.argmax(output_scores, 2)\n",
    "        for gold_sent, pred_sent in zip(labels, predicted_tags):\n",
    "            for gold_label, pred_label in zip(gold_sent, pred_sent):\n",
    "                if gold_label.item() != 0:\n",
    "                    total += 1\n",
    "                    if gold_label.item() == pred_label.item():\n",
    "                        match += 1\n",
    "    accuracy = match / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate on test data\n",
    "test_accuracy = evaluate_on_test(model, test_data, token_vocab, label_vocab)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
